# DECISÃƒO â€” LLM Local por padrÃ£o; sem fallback cloud
**Status:** ğŸŸ¢ VIGENTE  
**Data:** 2026-01-12  
**Autoridade:** ConstituiÃ§Ã£o Aurora (derivada)

---

## DecisÃ£o
1) Qualquer chat interno, rotas operacionais e ferramentas de agente devem usar **LLM/SLM local** por padrÃ£o (ex.: Ollama, LM Studio, servidor local).
2) Provedor cloud (OpenAI/Anthropic/etc.) Ã© **proibido por padrÃ£o**.
3) Se o provider for desconhecido: **falhar explicitamente**.
4) **Sem persistÃªncia automÃ¡tica** de conversas por padrÃ£o (side-effects = zero), exceto quando existir OS especÃ­fica de persistÃªncia + evidÃªncia.
5) Logs devem evitar conteÃºdo sensÃ­vel; registrar apenas metadados operacionais quando necessÃ¡rio.

---

## Racional

- Soberania cognitiva
- ReduÃ§Ã£o de custo
- Evitar dependÃªncia de credenciais
- Auditabilidade e previsibilidade
